{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "from data_adaptor import DataAdaptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THIS make funtion for taking finetuning to alpaca\n",
    "# run data_genrator but change adaptor\n",
    "# custom function for alpaca finetuning in adaptor\n",
    "# maybe don't use finetuning data in google drive, sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EXAMPLES = -1\n",
    "SPLIT = 'test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2WikiMultihopQA\n",
    "Authors only use question-answer information. The context is not provided.\n",
    "> \"we use only the question-answer pairs from these datasets, not any passages of relevant text that they contain. These datasets both contain 2-hop compositional questions sourced from facts that appear in Wikipedia articles.\" - Press, et al.\n",
    "\n",
    "Authors do not use this dataset to measure compositionality gap which requires known sub-questions and answers to measure.\n",
    "> \"Note that the rest of this section shows that elicitive prompts improve performance but does not show that they narrow the compositionality gap since we lack sub-questions for datasets other than CC.\" - Press, et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loaders import load_2WikiMultihopQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "wiki_sample = load_2WikiMultihopQA(n_examples=N_EXAMPLES, split=SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12576"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"_id\": \"8a65901a0bdb11eba7f7acde48001122\",\n",
      "    \"type\": \"compositional\",\n",
      "    \"question\": \"Which country the director of film One Law For The Woman is from?\",\n",
      "    \"context\": [\n",
      "        [\n",
      "            \"Dell Henderson\",\n",
      "            [\n",
      "                \"George Delbert \\\"Dell\\\" Henderson (July 5, 1877 \\u2013 December 2, 1956) was a Canadian-American actor, director, and writer.\",\n",
      "                \"He began his long and prolific film career in the early days of silent film.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"Inez Mee Boren\",\n",
      "            [\n",
      "                \"Inez Mee Boren( born November 2, 1880) was the director of the Woman's City Club of Oakland.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"John Farrell (businessman)\",\n",
      "            [\n",
      "                \"John Farrell is the director of YouTube in Latin America.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"Peter Levin\",\n",
      "            [\n",
      "                \"Peter Levin is an American director of film, television and theatre.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"Michael Govan\",\n",
      "            [\n",
      "                \"Michael Govan( born 1963) is the director of the Los Angeles County Museum of Art since 2006.\",\n",
      "                \"Prior to this, Govan worked as the director of the Dia Art Foundation in New York City.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"John Donatich\",\n",
      "            [\n",
      "                \"John Donatich is the Director of Yale University Press.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"Ian Barry (director)\",\n",
      "            [\n",
      "                \"Ian Barry is an Australian director of film and TV.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"One Law for Both\",\n",
      "            [\n",
      "                \"One Law for Both is a 1917 American silent drama film directed by Ivan Abramson.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"Dana Blankstein\",\n",
      "            [\n",
      "                \"Dana Blankstein- Cohen( born March 3, 1981) is the director of the Israeli Academy of Film and Television.\",\n",
      "                \"She is a film director, and an Israeli culture entrepreneur.\"\n",
      "            ]\n",
      "        ],\n",
      "        [\n",
      "            \"One Law for the Woman\",\n",
      "            [\n",
      "                \"One Law for the Woman is a 1924 American silent western film directed by Dell Henderson and starring Cullen Landis, Mildred Harris and Cecil Spooner.\"\n",
      "            ]\n",
      "        ]\n",
      "    ],\n",
      "    \"supporting_facts\": [],\n",
      "    \"evidences\": [],\n",
      "    \"answer\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print the first example in pretty format\n",
    "print(json.dumps(wiki_sample[0], indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the evidence required to answer the question. It's possible we create prompt examples using these evidences to insert into the fine-tuning set. For example,\n",
    "\n",
    "**Examplar:**\n",
    "```\n",
    "Question: Are director of film Move (1970 Film) and director of film Méditerranée (1963 Film) from the same country?\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Move (1970 film)?\n",
    "Intermediate answer: Stuart Rosenberg.\n",
    "Follow up: Who is the director of Méditerranée (1963 film)?\n",
    "Intermediate answer: Jean-Daniel Pollet\n",
    "Follow up: What is the country of citizenship of Stuart Rosenberg?\n",
    "Intermediate answer: American\n",
    "Follow up: What is the country of citizenship of Jean-Daniel Pollet?\n",
    "Intermediate answer: French\n",
    "So the final answer is: no\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sample[0]['supporting_facts']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting to Self-Ask Examplar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_adaptor = DataAdaptor(dataset=\"2WikiMultihopQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wiki_examplars = wiki_adaptor.generate_examplars(wiki_sample, strategy=\"self-ask\")\n",
    "#for examplar in wiki_examplars:\n",
    " #   print(examplar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting to Self-Ask Training Example\n",
    "We can augment the target texts in the dataset with the self-ask rationale to fine-tune a language model to generate text with the self-ask rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wiki_training_examples = wiki_adaptor.generate_training_examples(wiki_sample, strategy=\"self-ask\")\n",
    "#for training_example in wiki_training_examples[:5]:\n",
    " #   print(json.dumps(training_example, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(wiki_training_examples[0][\"prompt\"])\n",
    "# print(wiki_training_examples[0][\"target\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Prompting Training Examples\n",
    "Simply provide the facts and ask the question. No thought variable or rationale involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 2WikiMultihopQA direct training examples: 100%|██████████████████████████| 12576/12576 [00:00<00:00, 317255.70it/s]\n",
      "Structuring 2WikiMultihopQA direct training examples: 100%|██████████████████████████| 12576/12576 [00:00<00:00, 12621.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12576"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direct_training_examples = wiki_adaptor.generate_training_examples(\n",
    "    wiki_sample,\n",
    "    strategy=\"direct\"\n",
    ")\n",
    "len(direct_training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Augmented Prompt ---------\n",
      "\n",
      "Question: Which country the director of film One Law For The Woman is from?\n",
      "Answer:\n",
      "--------- Target ---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--------- Augmented Prompt ---------\")\n",
    "print(direct_training_examples[0][\"prompt\"])\n",
    "print(\"--------- Target ---------\")\n",
    "print(direct_training_examples[0][\"target\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment with In-Context Examplars and Self-Ask Rationale Targets\n",
    "We can combine the two above to create an augmented fine-tuning dataset:\n",
    "1. Prompt text has in-context examplars\n",
    "2. Target text has the self-ask rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_examplars = wiki_examplars[:4]\n",
    "# augmented_example = wiki_adaptor.generate_training_examples(\n",
    "#     wiki_sample[0], \n",
    "#     strategy=\"self-ask\", \n",
    "#     examplars=training_examplars\n",
    "#     )[0]\n",
    "# print(\"--------- Augmented Prompt ---------\")\n",
    "# print(augmented_example[\"prompt\"])\n",
    "# print(\"--------- Target ---------\")\n",
    "# print(augmented_example[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_examplars = wiki_examplars[:2]\n",
    "# augmented_examples = wiki_adaptor.generate_training_examples(\n",
    "#     wiki_sample, \n",
    "#     strategy=\"self-ask\",\n",
    "#     examplars=training_examplars\n",
    "#     )\n",
    "\n",
    "# # look at token counts in prompt (context size)\n",
    "# print(\"context size\")\n",
    "# for example in augmented_examples:\n",
    "#     print(example[\"num_prompt_tokens\"])\n",
    "\n",
    "# # look at token counts\n",
    "# print(\"total tokens\")\n",
    "# for example in augmented_examples:\n",
    "#     print(example[\"num_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(augmented_examples[1][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format for alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '\\nQuestion: Which country the director of film One Law For The Woman is from?\\nAnswer:',\n",
       " 'target': '',\n",
       " 'num_prompt_tokens': 18,\n",
       " 'num_target_tokens': 0,\n",
       " 'num_tokens': 18}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting examples for alpaca:   0%|                                                               | 0/12576 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm(direct_training_examples, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFormatting examples for alpaca\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     prompt_split \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     current_example \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstruction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mprompt_split\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt_split[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m      8\u001b[0m     alpaca_training_examples\u001b[38;5;241m.\u001b[39mappend(current_example)\n\u001b[1;32m     10\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(alpaca_training_examples[:\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "alpaca_training_examples = []\n",
    "#for example in direct_training_examples:\n",
    "for example in tqdm(direct_training_examples, desc=\"Formatting examples for alpaca\"):\n",
    "    prompt_split = example['prompt'].split('\\n\\n')\n",
    "    current_example = {'instruction': prompt_split[1],\n",
    "        'input': prompt_split[0],\n",
    "        'output': example['target']}\n",
    "    alpaca_training_examples.append(current_example)\n",
    "\n",
    "pprint.pprint(alpaca_training_examples[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/Alpaca-LoRa/{SPLIT}_alpaca.json', 'w') as f:\n",
    "    json.dump(alpaca_training_examples, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
