{"cells":[{"cell_type":"markdown","metadata":{"id":"fA4ROD7HaSJL"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"LK63fBfDbM5D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704391827670,"user_tz":360,"elapsed":49885,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"cdcd8c1b-8875-4952-ffef-3c0cdd074863"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n","Collecting loguru\n","  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: loguru\n","Successfully installed loguru-0.7.2\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.23.5)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.1)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=d9de32eb742ec9352e99cfa4bc0912f3b16a8c8a416a397faac5abd5e126760f\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n","Collecting peft\n","  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.35.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n","Collecting accelerate>=0.21.0 (from peft)\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.1)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Installing collected packages: accelerate, peft\n","Successfully installed accelerate-0.25.0 peft-0.7.1\n"]}],"source":["# !pip install pytest\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install tokenizers\n","!pip install nltk\n","!pip install loguru\n","!pip install rouge-score\n","!pip install peft"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"fO0ZlSt6vUQr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704054529585,"user_tz":360,"elapsed":35,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"54dbbc2e-1b34-4091-f3e7-793abdde1ff6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Dec 31 20:28:49 2023       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"mMShMWVHvT-r"},"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22914,"status":"ok","timestamp":1704391852506,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"},"user_tz":360},"id":"1djF5RoWXDce","outputId":"e3379475-06cb-422d-8aca-90796c1ff9c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1795,"status":"ok","timestamp":1704391854296,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"},"user_tz":360},"id":"Hp1iPONkluJM","outputId":"f3bec8a7-62d7-4500-9115-0aab71268ce8"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/projects/compositional-reasoning-finetuning\n"]}],"source":["%cd drive/MyDrive/projects/compositional-reasoning-finetuning"]},{"cell_type":"markdown","source":["# Run Evaluation"],"metadata":{"id":"EHxz0wAdNimM"}},{"cell_type":"code","source":["!python3 evaluation.py --model='t5-small' --strategy='self_ask' --no-examplars --no-answer_first --no-random_facts --no-load_checkpoint  --size=-1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NkK2EtL9NhtG","executionInfo":{"status":"ok","timestamp":1704057775810,"user_tz":360,"elapsed":3174834,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"c273c773-9632-47f3-b1f5-8fe0c376dd1d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-31 20:30:05.035491: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-31 20:30:05.035545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-31 20:30:05.036839: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-31 20:30:05.044181: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-31 20:30:06.099084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2023-12-31 20:30:17.229\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m543\u001b[0m - \u001b[34m\u001b[1mExamplars parameter -> False\u001b[0m\n","\u001b[32m2023-12-31 20:30:17.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m544\u001b[0m - \u001b[34m\u001b[1mCheckpoint loader parameter -> False\u001b[0m\n","\u001b[32m2023-12-31 20:30:17.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_t5_tokenizer\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mLoading tokenizer t5-small with max length of 300\u001b[0m\n","tokenizer_config.json: 100% 2.32k/2.32k [00:00<00:00, 11.1MB/s]\n","spiece.model: 100% 792k/792k [00:00<00:00, 73.4MB/s]\n","tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 5.62MB/s]\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2023-12-31 20:30:19.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1mTest set pointing to file: data/MultihopEvaluation/self_ask-answer_first=False-random_facts=False-without-examplars.json\u001b[0m\n","\u001b[32m2023-12-31 20:30:19.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m551\u001b[0m - \u001b[1mResponses will be saved to: results/t5-small-self_ask-answer_first=False-random_facts=False-without-examplars-responses.json\u001b[0m\n","\u001b[32m2023-12-31 20:30:19.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mResults will be saved to: results/t5-small-self_ask-answer_first=False-random_facts=False-without-examplars-results.json\u001b[0m\n","\u001b[32m2023-12-31 20:30:23.794\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m564\u001b[0m - \u001b[1mLoading t5-small model\u001b[0m\n","config.json: 100% 1.21k/1.21k [00:00<00:00, 5.31MB/s]\n","model.safetensors: 100% 242M/242M [00:00<00:00, 254MB/s]\n","2023-12-31 20:30:25.844909: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.141950: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.142353: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.143186: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.143553: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.143805: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.268459: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.268753: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.268889: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2023-12-31 20:30:26.268973: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 20:30:26.269252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n","\n","All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","\u001b[32m2023-12-31 20:30:35.477\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mLoading finetuned model weights from: models/t5-small-self_ask-answer_first=False-random_facts=False.h5\u001b[0m\n","\u001b[32m2023-12-31 20:30:43.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_progress\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mNo existing progress detected... starting from scratch.\u001b[0m\n","\u001b[32m2023-12-31 20:30:43.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m569\u001b[0m - \u001b[1mCollecting model responses...\u001b[0m\n","  0% 0/84 [00:00<?, ?it/s]2023-12-31 20:30:46.663450: I external/local_xla/xla/service/service.cc:168] XLA service 0x5bae92292a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2023-12-31 20:30:46.663505: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2023-12-31 20:30:46.681941: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2023-12-31 20:30:46.718709: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1704054646.782430    2438 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","100% 84/84 [51:53<00:00, 37.07s/it]\n","\u001b[32m2023-12-31 21:22:37.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m599\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2023-12-31 21:22:37.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mComputing results...\u001b[0m\n","8354it [00:15, 537.95it/s]\n","\u001b[32m2023-12-31 21:22:52.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m613\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2023-12-31 21:22:52.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mstore_evaluation_results\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mStoring evaluation results at results/t5-small-self_ask-answer_first=False-random_facts=False-without-examplars-results.json\u001b[0m\n"]}]},{"cell_type":"code","source":["!python3 evaluation.py --model='t5-small' --strategy='self_ask' --examplars --no-answer_first --no-random_facts --no-load_checkpoint  --size=-1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcNmrUYBxZgb","executionInfo":{"status":"ok","timestamp":1704060054159,"user_tz":360,"elapsed":2045078,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"d4c3af9b-8c5d-4390-ef41-5cccfcd2dd45"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-31 21:26:49.711322: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-31 21:26:49.711371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-31 21:26:49.712729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-31 21:26:49.720011: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-31 21:26:50.801483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2023-12-31 21:26:55.316\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m543\u001b[0m - \u001b[34m\u001b[1mExamplars parameter -> True\u001b[0m\n","\u001b[32m2023-12-31 21:26:55.317\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m544\u001b[0m - \u001b[34m\u001b[1mCheckpoint loader parameter -> False\u001b[0m\n","\u001b[32m2023-12-31 21:26:55.317\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_t5_tokenizer\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mLoading tokenizer t5-small with max length of 300\u001b[0m\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2023-12-31 21:26:55.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1mTest set pointing to file: data/MultihopEvaluation/self_ask-answer_first=False-random_facts=False-with-examplars.json\u001b[0m\n","\u001b[32m2023-12-31 21:26:55.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m551\u001b[0m - \u001b[1mResponses will be saved to: results/t5-small-self_ask-answer_first=False-random_facts=False-with-examplars-responses.json\u001b[0m\n","\u001b[32m2023-12-31 21:26:55.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mResults will be saved to: results/t5-small-self_ask-answer_first=False-random_facts=False-with-examplars-results.json\u001b[0m\n","\u001b[32m2023-12-31 21:26:57.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m564\u001b[0m - \u001b[1mLoading t5-small model\u001b[0m\n","2023-12-31 21:26:57.331551: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.341276: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.341585: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.342407: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.342669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.342869: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.458834: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.459211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.459399: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2023-12-31 21:26:57.459528: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 21:26:57.459799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n","\n","All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","\u001b[32m2023-12-31 21:27:07.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mLoading finetuned model weights from: models/t5-small-self_ask-answer_first=False-random_facts=False.h5\u001b[0m\n","\u001b[32m2023-12-31 21:27:08.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_progress\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mNo existing progress detected... starting from scratch.\u001b[0m\n","\u001b[32m2023-12-31 21:27:08.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m569\u001b[0m - \u001b[1mCollecting model responses...\u001b[0m\n","  0% 0/84 [00:00<?, ?it/s]2023-12-31 21:27:12.345413: I external/local_xla/xla/service/service.cc:168] XLA service 0x58fff8072310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2023-12-31 21:27:12.345462: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2023-12-31 21:27:12.351638: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2023-12-31 21:27:12.371912: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1704058032.458243   17279 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","100% 84/84 [33:27<00:00, 23.89s/it]\n","\u001b[32m2023-12-31 22:00:35.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m599\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2023-12-31 22:00:35.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mComputing results...\u001b[0m\n","8354it [00:16, 499.77it/s]\n","\u001b[32m2023-12-31 22:00:51.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m613\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2023-12-31 22:00:51.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mstore_evaluation_results\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mStoring evaluation results at results/t5-small-self_ask-answer_first=False-random_facts=False-with-examplars-results.json\u001b[0m\n"]}]},{"cell_type":"code","source":["!python3 evaluation.py --model='t5-small' --strategy='self_ask' --examplars --answer_first --no-random_facts --no-load_checkpoint  --size=-1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CchiGgv1_TsV","executionInfo":{"status":"ok","timestamp":1704063771560,"user_tz":360,"elapsed":2111610,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"7b8fea45-43bd-4121-b858-d63ff435df89"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-31 22:27:40.643447: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-31 22:27:40.643512: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-31 22:27:40.645050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-31 22:27:40.656302: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-31 22:27:41.743087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2023-12-31 22:27:46.381\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m543\u001b[0m - \u001b[34m\u001b[1mExamplars parameter -> True\u001b[0m\n","\u001b[32m2023-12-31 22:27:46.382\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m544\u001b[0m - \u001b[34m\u001b[1mCheckpoint loader parameter -> False\u001b[0m\n","\u001b[32m2023-12-31 22:27:46.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_t5_tokenizer\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mLoading tokenizer t5-small with max length of 300\u001b[0m\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2023-12-31 22:27:46.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1mTest set pointing to file: data/MultihopEvaluation/self_ask-answer_first=True-random_facts=False-with-examplars.json\u001b[0m\n","\u001b[32m2023-12-31 22:27:46.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m551\u001b[0m - \u001b[1mResponses will be saved to: results/t5-small-self_ask-answer_first=True-random_facts=False-with-examplars-responses.json\u001b[0m\n","\u001b[32m2023-12-31 22:27:46.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mResults will be saved to: results/t5-small-self_ask-answer_first=True-random_facts=False-with-examplars-results.json\u001b[0m\n","\u001b[32m2023-12-31 22:27:48.990\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m564\u001b[0m - \u001b[1mLoading t5-small model\u001b[0m\n","2023-12-31 22:27:49.302513: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.312890: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.313287: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.314129: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.314407: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.314672: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.434143: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.434518: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.434711: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2023-12-31 22:27:49.434798: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-12-31 22:27:49.434969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n","\n","All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","\u001b[32m2023-12-31 22:27:59.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mLoading finetuned model weights from: models/t5-small-self_ask-answer_first=True-random_facts=False.h5\u001b[0m\n","\u001b[32m2023-12-31 22:28:07.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_progress\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mNo existing progress detected... starting from scratch.\u001b[0m\n","\u001b[32m2023-12-31 22:28:07.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m569\u001b[0m - \u001b[1mCollecting model responses...\u001b[0m\n","  0% 0/84 [00:00<?, ?it/s]2023-12-31 22:28:11.757251: I external/local_xla/xla/service/service.cc:168] XLA service 0x5647150bec40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2023-12-31 22:28:11.757302: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2023-12-31 22:28:11.761799: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2023-12-31 22:28:11.774838: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1704061691.831538   32984 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","100% 84/84 [34:21<00:00, 24.54s/it]\n","\u001b[32m2023-12-31 23:02:29.235\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m599\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2023-12-31 23:02:29.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mComputing results...\u001b[0m\n","8354it [00:19, 428.88it/s]\n","\u001b[32m2023-12-31 23:02:48.741\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m613\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2023-12-31 23:02:48.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mstore_evaluation_results\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mStoring evaluation results at results/t5-small-self_ask-answer_first=True-random_facts=False-with-examplars-results.json\u001b[0m\n"]}]},{"cell_type":"code","source":["!python3 evaluation.py --model='t5-small' --strategy='self_ask' --no-examplars --answer_first --no-random_facts --no-load_checkpoint  --size=-1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5XUo2gVO2MQ","executionInfo":{"status":"ok","timestamp":1704135731189,"user_tz":360,"elapsed":2793613,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"02a3508a-0174-425e-a1f5-b2707babfb62"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-01 18:15:39.870318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-01 18:15:39.870365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-01 18:15:39.871698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-01 18:15:39.882004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-01 18:15:40.992731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2024-01-01 18:15:53.795\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m543\u001b[0m - \u001b[34m\u001b[1mExamplars parameter -> False\u001b[0m\n","\u001b[32m2024-01-01 18:15:53.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m544\u001b[0m - \u001b[34m\u001b[1mCheckpoint loader parameter -> False\u001b[0m\n","\u001b[32m2024-01-01 18:15:53.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_t5_tokenizer\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mLoading tokenizer t5-small with max length of 300\u001b[0m\n","tokenizer_config.json: 100% 2.32k/2.32k [00:00<00:00, 11.1MB/s]\n","spiece.model: 100% 792k/792k [00:00<00:00, 12.0MB/s]\n","tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 11.4MB/s]\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2024-01-01 18:15:54.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1mTest set pointing to file: data/MultihopEvaluation/self_ask-answer_first=True-random_facts=False-without-examplars.json\u001b[0m\n","\u001b[32m2024-01-01 18:15:54.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m551\u001b[0m - \u001b[1mResponses will be saved to: results/t5-small-self_ask-answer_first=True-random_facts=False-without-examplars-responses.json\u001b[0m\n","\u001b[32m2024-01-01 18:15:54.974\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mResults will be saved to: results/t5-small-self_ask-answer_first=True-random_facts=False-without-examplars-results.json\u001b[0m\n","\u001b[32m2024-01-01 18:15:56.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m564\u001b[0m - \u001b[1mLoading t5-small model\u001b[0m\n","config.json: 100% 1.21k/1.21k [00:00<00:00, 6.96MB/s]\n","model.safetensors: 100% 242M/242M [00:01<00:00, 215MB/s]\n","2024-01-01 18:15:57.696582: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.016183: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.016565: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.017468: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.017786: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.018019: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.247573: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.247929: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.248080: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2024-01-01 18:15:58.248178: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 18:15:58.248349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n","\n","All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","\u001b[32m2024-01-01 18:16:09.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mLoading finetuned model weights from: models/t5-small-self_ask-answer_first=True-random_facts=False.h5\u001b[0m\n","\u001b[32m2024-01-01 18:16:12.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_progress\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mNo existing progress detected... starting from scratch.\u001b[0m\n","\u001b[32m2024-01-01 18:16:12.895\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m569\u001b[0m - \u001b[1mCollecting model responses...\u001b[0m\n","  0% 0/84 [00:00<?, ?it/s]2024-01-01 18:16:16.587535: I external/local_xla/xla/service/service.cc:168] XLA service 0x5c2bbcecea60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2024-01-01 18:16:16.587586: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2024-01-01 18:16:16.606666: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2024-01-01 18:16:16.649228: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1704132976.714404    1298 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","100% 84/84 [45:38<00:00, 32.60s/it]\n","\u001b[32m2024-01-01 19:01:51.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m599\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2024-01-01 19:01:51.180\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mComputing results...\u001b[0m\n","8354it [00:16, 493.26it/s]\n","\u001b[32m2024-01-01 19:02:08.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m613\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2024-01-01 19:02:08.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mstore_evaluation_results\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mStoring evaluation results at results/t5-small-self_ask-answer_first=True-random_facts=False-without-examplars-results.json\u001b[0m\n"]}]},{"cell_type":"code","source":["!python3 evaluation.py --model='t5-small' --strategy='self_ask' --examplars --no-answer_first --random_facts --no-load_checkpoint  --size=-1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YMyB_pFskH1z","executionInfo":{"status":"ok","timestamp":1704140730916,"user_tz":360,"elapsed":685181,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"4ac7f538-e28c-475c-d048-bf78b801dac0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-01 19:47:32.351426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-01 19:47:32.351478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-01 19:47:32.353090: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-01 19:47:32.360601: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-01 19:47:33.428717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2024-01-01 19:47:39.309\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m543\u001b[0m - \u001b[34m\u001b[1mExamplars parameter -> True\u001b[0m\n","\u001b[32m2024-01-01 19:47:39.310\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m544\u001b[0m - \u001b[34m\u001b[1mCheckpoint loader parameter -> False\u001b[0m\n","\u001b[32m2024-01-01 19:47:39.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_t5_tokenizer\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mLoading tokenizer t5-small with max length of 300\u001b[0m\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2024-01-01 19:47:39.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1mTest set pointing to file: data/MultihopEvaluation/self_ask-answer_first=False-random_facts=True-with-examplars.json\u001b[0m\n","\u001b[32m2024-01-01 19:47:39.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m551\u001b[0m - \u001b[1mResponses will be saved to: results/t5-small-self_ask-answer_first=False-random_facts=True-with-examplars-responses.json\u001b[0m\n","\u001b[32m2024-01-01 19:47:39.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mResults will be saved to: results/t5-small-self_ask-answer_first=False-random_facts=True-with-examplars-results.json\u001b[0m\n","\u001b[32m2024-01-01 19:47:40.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m564\u001b[0m - \u001b[1mLoading t5-small model\u001b[0m\n","2024-01-01 19:47:40.610476: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.617682: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.617962: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.618786: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.619061: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.619296: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.807840: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.808234: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.808435: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2024-01-01 19:47:40.808558: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-01 19:47:40.808777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n","\n","All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","\u001b[32m2024-01-01 19:47:49.996\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mLoading finetuned model weights from: models/t5-small-self_ask-answer_first=False-random_facts=True.h5\u001b[0m\n","\u001b[32m2024-01-01 19:47:53.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_progress\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mNo existing progress detected... starting from scratch.\u001b[0m\n","\u001b[32m2024-01-01 19:47:53.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m569\u001b[0m - \u001b[1mCollecting model responses...\u001b[0m\n","  0% 0/84 [00:00<?, ?it/s]2024-01-01 19:47:57.988081: I external/local_xla/xla/service/service.cc:168] XLA service 0x5d12a09b4d50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2024-01-01 19:47:57.988130: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2024-01-01 19:47:57.992772: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2024-01-01 19:47:58.006516: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1704138478.063735   24509 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","100% 84/84 [37:20<00:00, 26.67s/it]\n","\u001b[32m2024-01-01 20:25:13.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m599\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2024-01-01 20:25:13.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mComputing results...\u001b[0m\n","8354it [00:15, 530.82it/s]\n","\u001b[32m2024-01-01 20:25:29.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m613\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2024-01-01 20:25:29.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mstore_evaluation_results\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mStoring evaluation results at results/t5-small-self_ask-answer_first=False-random_facts=True-with-examplars-results.json\u001b[0m\n"]}]},{"cell_type":"code","source":["!python3 evaluation.py --model='t5-small' --strategy='self_ask' --no-examplars --no-answer_first --random_facts --no-load_checkpoint  --size=-1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyRfPM80xCSi","executionInfo":{"status":"ok","timestamp":1704395276204,"user_tz":360,"elapsed":3079425,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"418f8265-4419-4189-f607-fda153210eb8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-04 18:16:37.638268: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-04 18:16:37.638321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-04 18:16:37.639679: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-04 18:16:37.646871: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-04 18:16:38.730949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2024-01-04 18:16:43.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m543\u001b[0m - \u001b[34m\u001b[1mExamplars parameter -> False\u001b[0m\n","\u001b[32m2024-01-04 18:16:43.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m544\u001b[0m - \u001b[34m\u001b[1mCheckpoint loader parameter -> False\u001b[0m\n","\u001b[32m2024-01-04 18:16:43.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_t5_tokenizer\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mLoading tokenizer t5-small with max length of 300\u001b[0m\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2024-01-04 18:16:43.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1mTest set pointing to file: data/MultihopEvaluation/self_ask-answer_first=False-random_facts=True-without-examplars.json\u001b[0m\n","\u001b[32m2024-01-04 18:16:43.877\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m551\u001b[0m - \u001b[1mResponses will be saved to: results/t5-small-self_ask-answer_first=False-random_facts=True-without-examplars-responses.json\u001b[0m\n","\u001b[32m2024-01-04 18:16:43.878\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mResults will be saved to: results/t5-small-self_ask-answer_first=False-random_facts=True-without-examplars-results.json\u001b[0m\n","\u001b[32m2024-01-04 18:16:45.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m564\u001b[0m - \u001b[1mLoading t5-small model\u001b[0m\n","config.json: 100% 1.21k/1.21k [00:00<00:00, 6.18MB/s]\n","model.safetensors: 100% 242M/242M [00:02<00:00, 113MB/s]\n","2024-01-04 18:16:48.243308: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.574576: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.575012: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.575913: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.576237: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.576480: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.734139: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.734479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.734669: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2024-01-04 18:16:48.734755: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 18:16:48.734936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n","\n","All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","\u001b[32m2024-01-04 18:16:59.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mLoading finetuned model weights from: models/t5-small-self_ask-answer_first=False-random_facts=True.h5\u001b[0m\n","\u001b[32m2024-01-04 18:17:06.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_progress\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mNo existing progress detected... starting from scratch.\u001b[0m\n","\u001b[32m2024-01-04 18:17:06.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m569\u001b[0m - \u001b[1mCollecting model responses...\u001b[0m\n","  0% 0/84 [00:00<?, ?it/s]2024-01-04 18:17:09.367529: I external/local_xla/xla/service/service.cc:168] XLA service 0x577073b2f080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2024-01-04 18:17:09.367576: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2024-01-04 18:17:09.388655: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2024-01-04 18:17:09.428132: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1704392229.493813    2344 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","100% 84/84 [50:35<00:00, 36.14s/it]\n","\u001b[32m2024-01-04 19:07:41.716\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m599\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2024-01-04 19:07:41.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mComputing results...\u001b[0m\n","8354it [00:12, 679.02it/s]\n","\u001b[32m2024-01-04 19:07:54.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m613\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2024-01-04 19:07:54.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mstore_evaluation_results\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mStoring evaluation results at results/t5-small-self_ask-answer_first=False-random_facts=True-without-examplars-results.json\u001b[0m\n"]}]},{"cell_type":"markdown","source":["!python3 evaluation.py --model='t5-small' --strategy='direct' --no-examplars --no-answer_first --random_facts --no-load_checkpoint  --size=-1"],"metadata":{"id":"mwXbuB2AqdXC"}},{"cell_type":"code","source":["!python3 evaluation.py --model='t5-small' --strategy='direct' --no-examplars --no-answer_first --no-random_facts --no-load_checkpoint  --size=-1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4xmB7YYqhyF","executionInfo":{"status":"ok","timestamp":1704395614671,"user_tz":360,"elapsed":317257,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"}},"outputId":"ed35bed3-2475-4f34-c51e-9f65425432ce"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-04 19:08:17.699902: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-04 19:08:17.699948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-04 19:08:17.701233: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-04 19:08:17.707988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-04 19:08:18.770719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[32m2024-01-04 19:08:23.152\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m543\u001b[0m - \u001b[34m\u001b[1mExamplars parameter -> False\u001b[0m\n","\u001b[32m2024-01-04 19:08:23.153\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m544\u001b[0m - \u001b[34m\u001b[1mCheckpoint loader parameter -> False\u001b[0m\n","\u001b[32m2024-01-04 19:08:23.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_set_t5_tokenizer\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mLoading tokenizer t5-small with max length of 300\u001b[0m\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","\u001b[32m2024-01-04 19:08:23.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m550\u001b[0m - \u001b[1mTest set pointing to file: data/MultihopEvaluation/direct-answer_first=False-random_facts=False-without-examplars.json\u001b[0m\n","\u001b[32m2024-01-04 19:08:23.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m551\u001b[0m - \u001b[1mResponses will be saved to: results/t5-small-direct-answer_first=False-random_facts=False-without-examplars-responses.json\u001b[0m\n","\u001b[32m2024-01-04 19:08:23.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m552\u001b[0m - \u001b[1mResults will be saved to: results/t5-small-direct-answer_first=False-random_facts=False-without-examplars-results.json\u001b[0m\n","\u001b[32m2024-01-04 19:08:24.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m564\u001b[0m - \u001b[1mLoading t5-small model\u001b[0m\n","2024-01-04 19:08:25.045675: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.053062: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.053337: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.054038: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.054305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.054510: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.145342: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.145656: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.145815: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2024-01-04 19:08:25.145919: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-01-04 19:08:25.146132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n","\n","All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","\u001b[32m2024-01-04 19:08:35.263\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mLoading finetuned model weights from: models/t5-small-direct-answer_first=False-random_facts=False.h5\u001b[0m\n","\u001b[32m2024-01-04 19:08:40.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_progress\u001b[0m:\u001b[36m474\u001b[0m - \u001b[1mNo existing progress detected... starting from scratch.\u001b[0m\n","\u001b[32m2024-01-04 19:08:40.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m569\u001b[0m - \u001b[1mCollecting model responses...\u001b[0m\n","  0% 0/84 [00:00<?, ?it/s]2024-01-04 19:08:43.883761: I external/local_xla/xla/service/service.cc:168] XLA service 0x5a5b9490b760 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2024-01-04 19:08:43.883800: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n","2024-01-04 19:08:43.889669: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2024-01-04 19:08:43.908435: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1704395323.991376   15730 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","100% 84/84 [04:50<00:00,  3.46s/it]\n","\u001b[32m2024-01-04 19:13:30.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m599\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2024-01-04 19:13:30.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mComputing results...\u001b[0m\n","8354it [00:01, 5727.73it/s]\n","\u001b[32m2024-01-04 19:13:32.305\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m613\u001b[0m - \u001b[1mdone\u001b[0m\n","\u001b[32m2024-01-04 19:13:32.306\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mstore_evaluation_results\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mStoring evaluation results at results/t5-small-direct-answer_first=False-random_facts=False-without-examplars-results.json\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"3FJsAxi3OdTb"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQLmQsT4QpVT"},"outputs":[],"source":["from evaluation import load_responses, EvaluationConfig\n","from data_loaders import load_TestData\n","import json\n","config = EvaluationConfig(\"flan-t5-small-direct\", examplars=False, data_path=\"data/MultihopEvaluation/\", results_path=\"results/\", create_tokenizer=False)\n","responses = load_responses(config)\n","test_set = load_TestData(config.generate_test_set_file(), n_examples=5)\n","with open(config.generate_results_file(), \"r\") as f:\n","    results = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1690057912041,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"},"user_tz":300},"id":"P4YSKMMUy23E","outputId":"5337c783-7f4a-42ad-8846-39d193a90faf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Example: 0\n","Facts:\n","Fact #0: (Wojna polsko-ruska) is a 2009 Polish film directed by Xawery Żuławski based on the novel Polish-Russian War under the white-red flag by Dorota Masłowska.\n","Fact #1: He is the son of actress Małgorzata Braunek and director Andrzej Żuławski.\n","\n","Question: Who is the mother of the director of film Polish-Russian War (Film)?\n","Answer:\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Reference text:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Małgorzata Braunek'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["True answer: Małgorzata Braunek\n","\n","--------\n","T5 response\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Response:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Magorzata Braunek'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Answer: Magorzata Braunek\n","--------\n","\n","\n","\n","Example: 1\n","Facts:\n","Fact #0: Blind Shaft is a 2003 film about a pair of brutal con artists operating in the illegal coal mines of present- day northern China.\n","Fact #1: The Mask of Fu Manchu is a 1932 pre-Code adventure film directed by Charles Brabin.\n","\n","Question: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\n","Answer:\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Reference text:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The Mask Of Fu Manchu'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["True answer: The Mask Of Fu Manchu\n","\n","--------\n","T5 response\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Response:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The Mask Of Fu Manchu'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Answer: The Mask Of Fu Manchu\n","--------\n","\n","\n","\n","Example: 2\n","Facts:\n","Fact #0: John was the second (but eldest surviving) son of Ernest I, Prince of Anhalt-Dessau, by his wife Margarete, daughter of Henry I, Duke of Münsterberg-Oels, and granddaughter of George of Poděbrady, King of Bohemia.\n","Fact #1: Ernest I, Prince of Anhalt-Dessau (died Dessau, 12 June 1516), was a German prince of the House of Ascania and ruler of the principality of Anhalt-Dessau.\n","\n","Question: When did John V, Prince Of Anhalt-Zerbst's father die?\n","Answer:\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Reference text:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'12 June 1516'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["True answer: 12 June 1516\n","\n","--------\n","T5 response\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Response:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'12 June 1516'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Answer: 12 June 1516\n","--------\n","\n","\n","\n","Example: 3\n","Facts:\n","Fact #0: Wearing Velvet Slippers under a Golden Umbrella (Pronounced as Katipa phanat see shwe htee hsaung) is a 1970 Burmese film directed by Maung Wunna starring Myat Mon, Myat Lay and Thet Naung.\n","Fact #1: Maung Wunna  was a two-time Myanmar Motion Picture Academy Awards-winning Burmese director and writer.\n","\n","Question: What is the award that the director of film Wearing Velvet Slippers Under A Golden Umbrella won?\n","Answer:\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Reference text:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Myanmar Motion Picture Academy Awards'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["True answer: Myanmar Motion Picture Academy Awards\n","\n","--------\n","T5 response\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Response:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Film-Fee'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Answer: Film-Fee\n","--------\n","\n","\n","\n","Example: 4\n","Facts:\n","Fact #0: Ronnie Rocket is an unfinished film project written by David Lynch, who also intended to direct it.\n","Fact #1: Born to a middle-class family in Missoula, Montana, Lynch spent his childhood traveling around the United States before he studied painting at the Pennsylvania Academy of Fine Arts in Philadelphia, where he first made the transition to producing short films.\n","\n","Question: Where was the director of film Ronnie Rocket born?\n","Answer:\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Reference text:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Missoula, Montana'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["True answer: Missoula, Montana\n","\n","--------\n","T5 response\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Response:'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Missoula, Montana'"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Answer: Missoula, Montana\n","--------\n","\n","\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'T5 Results'"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'accuracy': 0.9,\n"," 'F1-1': 0.9,\n"," 'F1-2': 0.8,\n"," 'bleu-1': 0.9,\n"," 'bleu-2': 0.8,\n"," 'rouge-1': 0.9,\n"," 'rouge-2': 0.8,\n"," 'rouge-L': 0.9}"]},"metadata":{},"output_type":"display_data"}],"source":["for idx in range(5):\n","    print(\"Example:\", idx)\n","    print(test_set[idx][\"prompt\"])\n","    display(\"Reference text:\", test_set[idx][\"target\"])\n","    print(\"True answer:\", test_set[idx][\"answer\"])\n","    print()\n","    print(\"--------\")\n","    print(\"T5 response\")\n","    display(\"Response:\", responses[idx][\"response\"])\n","    print(\"Answer:\", responses[idx][\"answer\"])\n","    print(\"--------\")\n","    print(\"\\n\\n\")\n","display(\"T5 Results\")\n","display(results[\"macro_results\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1690057933171,"user":{"displayName":"Richard Mathews","userId":"15085444626849988348"},"user_tz":300},"id":"lTBhfV2r88eL","outputId":"aa5c790a-55f1-45f5-d74a-970c9047008c"},"outputs":[{"data":{"text/html":["\n","\n","  <div id=\"df-0f2b6462-bc77-4ea5-8d76-612b64225855\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>correct</th>\n","      <th>bleu-1</th>\n","      <th>bleu-2</th>\n","      <th>rouge-1</th>\n","      <th>rouge-2</th>\n","      <th>rouge-L</th>\n","      <th>F1-1</th>\n","      <th>F1-2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>True</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f2b6462-bc77-4ea5-8d76-612b64225855')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-11400eb7-f3ea-490a-9536-73775922d10b\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11400eb7-f3ea-490a-9536-73775922d10b')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-11400eb7-f3ea-490a-9536-73775922d10b button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0f2b6462-bc77-4ea5-8d76-612b64225855 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0f2b6462-bc77-4ea5-8d76-612b64225855');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"],"text/plain":["   correct  bleu-1  bleu-2  rouge-1  rouge-2  rouge-L  F1-1  F1-2\n","0     True     1.0     1.0      1.0      1.0      1.0   1.0   1.0\n","1     True     1.0     1.0      1.0      1.0      1.0   1.0   1.0\n","2     True     1.0     1.0      1.0      1.0      1.0   1.0   1.0\n","3    False     0.0     0.0      0.0      0.0      0.0   0.0   0.0\n","4     True     1.0     1.0      1.0      1.0      1.0   1.0   1.0\n","5     True     1.0     1.0      1.0      1.0      1.0   1.0   1.0\n","6     True     1.0     1.0      1.0      1.0      1.0   1.0   1.0\n","7     True     1.0     0.0      1.0      0.0      1.0   1.0   0.0\n","8     True     1.0     1.0      1.0      1.0      1.0   1.0   1.0\n","9     True     1.0     1.0      1.0      1.0      1.0   1.0   1.0"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","pd.DataFrame(results[\"micro_results\"])"]},{"cell_type":"markdown","metadata":{"id":"MX5oxvFuMgd7"},"source":["# Macro Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H88ZwMgMMgd7","outputId":"fc540cce-0077-4964-c23a-c32a1531f05f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Finetune</th>\n","      <th>With Examplars</th>\n","      <th>Accuracy</th>\n","      <th>F1-1</th>\n","      <th>F1-2</th>\n","      <th>BLEU-1</th>\n","      <th>BLEU-2</th>\n","      <th>ROUGE-1</th>\n","      <th>ROUGE-2</th>\n","      <th>ROUGE-L</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>flan-t5-small</td>\n","      <td>Direct</td>\n","      <td>Y</td>\n","      <td>0.4703</td>\n","      <td>0.0783</td>\n","      <td>0.0475</td>\n","      <td>0.6961</td>\n","      <td>0.5026</td>\n","      <td>0.0422</td>\n","      <td>0.0253</td>\n","      <td>0.0417</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>flan-t5-small</td>\n","      <td>Direct</td>\n","      <td>N</td>\n","      <td>0.6618</td>\n","      <td>0.6771</td>\n","      <td>0.4182</td>\n","      <td>0.6791</td>\n","      <td>0.4180</td>\n","      <td>0.6914</td>\n","      <td>0.4285</td>\n","      <td>0.6911</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>flan-t5-small</td>\n","      <td>Self Ask</td>\n","      <td>Y</td>\n","      <td>0.7790</td>\n","      <td>0.9680</td>\n","      <td>0.9501</td>\n","      <td>0.9688</td>\n","      <td>0.9509</td>\n","      <td>0.9693</td>\n","      <td>0.9512</td>\n","      <td>0.9688</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>flan-t5-small</td>\n","      <td>Self Ask</td>\n","      <td>N</td>\n","      <td>0.7325</td>\n","      <td>0.9412</td>\n","      <td>0.9125</td>\n","      <td>0.9560</td>\n","      <td>0.9271</td>\n","      <td>0.9329</td>\n","      <td>0.9042</td>\n","      <td>0.9314</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>flan-t5-small</td>\n","      <td>N/A</td>\n","      <td>Y</td>\n","      <td>0.5362</td>\n","      <td>0.0802</td>\n","      <td>0.0452</td>\n","      <td>0.7743</td>\n","      <td>0.5416</td>\n","      <td>0.0429</td>\n","      <td>0.0239</td>\n","      <td>0.0427</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>flan-t5-small</td>\n","      <td>N/A</td>\n","      <td>N</td>\n","      <td>0.5169</td>\n","      <td>0.5313</td>\n","      <td>0.3360</td>\n","      <td>0.5361</td>\n","      <td>0.3416</td>\n","      <td>0.5516</td>\n","      <td>0.3440</td>\n","      <td>0.5512</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>t5-small</td>\n","      <td>Direct</td>\n","      <td>Y</td>\n","      <td>0.1058</td>\n","      <td>0.0178</td>\n","      <td>0.0089</td>\n","      <td>0.1937</td>\n","      <td>0.1241</td>\n","      <td>0.0094</td>\n","      <td>0.0046</td>\n","      <td>0.0093</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>t5-small</td>\n","      <td>Direct</td>\n","      <td>N</td>\n","      <td>0.7093</td>\n","      <td>0.7245</td>\n","      <td>0.4493</td>\n","      <td>0.7277</td>\n","      <td>0.4492</td>\n","      <td>0.7362</td>\n","      <td>0.4584</td>\n","      <td>0.7361</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>t5-small</td>\n","      <td>Self Ask</td>\n","      <td>Y</td>\n","      <td>0.7478</td>\n","      <td>0.9636</td>\n","      <td>0.9449</td>\n","      <td>0.9634</td>\n","      <td>0.9449</td>\n","      <td>0.9664</td>\n","      <td>0.9476</td>\n","      <td>0.9649</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>t5-small</td>\n","      <td>Self Ask</td>\n","      <td>N</td>\n","      <td>0.4093</td>\n","      <td>0.6588</td>\n","      <td>0.5631</td>\n","      <td>0.7159</td>\n","      <td>0.6194</td>\n","      <td>0.6574</td>\n","      <td>0.5591</td>\n","      <td>0.6393</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>t5-small</td>\n","      <td>N/A</td>\n","      <td>Y</td>\n","      <td>0.2479</td>\n","      <td>0.0481</td>\n","      <td>0.0214</td>\n","      <td>0.4271</td>\n","      <td>0.2470</td>\n","      <td>0.0266</td>\n","      <td>0.0116</td>\n","      <td>0.0260</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>t5-small</td>\n","      <td>N/A</td>\n","      <td>N</td>\n","      <td>0.3298</td>\n","      <td>0.3286</td>\n","      <td>0.1911</td>\n","      <td>0.3247</td>\n","      <td>0.1909</td>\n","      <td>0.3642</td>\n","      <td>0.2089</td>\n","      <td>0.3638</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Model  Finetune With Examplars  Accuracy    F1-1    F1-2  BLEU-1  \\\n","0   flan-t5-small    Direct              Y    0.4703  0.0783  0.0475  0.6961   \n","1   flan-t5-small    Direct              N    0.6618  0.6771  0.4182  0.6791   \n","2   flan-t5-small  Self Ask              Y    0.7790  0.9680  0.9501  0.9688   \n","3   flan-t5-small  Self Ask              N    0.7325  0.9412  0.9125  0.9560   \n","4   flan-t5-small       N/A              Y    0.5362  0.0802  0.0452  0.7743   \n","5   flan-t5-small       N/A              N    0.5169  0.5313  0.3360  0.5361   \n","6        t5-small    Direct              Y    0.1058  0.0178  0.0089  0.1937   \n","7        t5-small    Direct              N    0.7093  0.7245  0.4493  0.7277   \n","8        t5-small  Self Ask              Y    0.7478  0.9636  0.9449  0.9634   \n","9        t5-small  Self Ask              N    0.4093  0.6588  0.5631  0.7159   \n","10       t5-small       N/A              Y    0.2479  0.0481  0.0214  0.4271   \n","11       t5-small       N/A              N    0.3298  0.3286  0.1911  0.3247   \n","\n","    BLEU-2  ROUGE-1  ROUGE-2  ROUGE-L  \n","0   0.5026   0.0422   0.0253   0.0417  \n","1   0.4180   0.6914   0.4285   0.6911  \n","2   0.9509   0.9693   0.9512   0.9688  \n","3   0.9271   0.9329   0.9042   0.9314  \n","4   0.5416   0.0429   0.0239   0.0427  \n","5   0.3416   0.5516   0.3440   0.5512  \n","6   0.1241   0.0094   0.0046   0.0093  \n","7   0.4492   0.7362   0.4584   0.7361  \n","8   0.9449   0.9664   0.9476   0.9649  \n","9   0.6194   0.6574   0.5591   0.6393  \n","10  0.2470   0.0266   0.0116   0.0260  \n","11  0.1909   0.3642   0.2089   0.3638  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["import os, json\n","import pandas as pd\n","\n","path_to_json = './results'\n","\n","json_files = [pos_json for pos_json in sorted(os.listdir(path_to_json)) if pos_json.endswith('.json')]\n","\n","jsons_data = pd.DataFrame(columns=['Model', 'Finetune', 'With Examplars', 'Accuracy', 'F1-1', 'F1-2', 'BLEU-1', 'BLEU-2', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L'])\n","\n","for index, js in enumerate(json_files):\n","    with open(os.path.join(path_to_json, js)) as json_file:\n","        json_text = json.load(json_file)\n","\n","        # Get rid of unnecessary filename and extension.\n","        js = js.replace(\"-results.json\", \"\")\n","\n","        # Mark \"With Examplars\" column\n","        if (\"-with-examplars\" in js):\n","            model = js.replace(\"-with-examplars\", \"\")\n","            examplars = 'Y'\n","        else:\n","            model = js.replace(\"-without-examplars\", \"\")\n","            examplars = 'N'\n","\n","        # Mark \"Finetune\" column\n","        if (\"-direct\" in model):\n","            model = model.replace(\"-direct\", \"\")\n","            fine_tune = \"Direct\"\n","        elif (\"-self-ask\" in model):\n","            model = model.replace(\"-self-ask\", \"\")\n","            fine_tune = \"Self Ask\"\n","        else:\n","            fine_tune = \"N/A\"\n","\n","        # Build data frame row\n","        accuracy = json_text['macro_results']['accuracy']\n","        f1_1 = json_text['macro_results']['F1-1']\n","        f1_2 = json_text['macro_results']['F1-2']\n","        bleu_1 = json_text['macro_results']['bleu-1']\n","        bleu_2 = json_text['macro_results']['bleu-2']\n","        rouge_1 = json_text['macro_results']['rouge-1']\n","        rouge_2 = json_text['macro_results']['rouge-2']\n","        rouge_L = json_text['macro_results']['rouge-L']\n","\n","        # Add row to data frame\n","        jsons_data.loc[index] = [model, fine_tune, examplars, accuracy, f1_1, f1_2, bleu_1, bleu_2, rouge_1, rouge_2, rouge_L]\n","\n","# print result\n","jsons_data\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xwbqkXOcMgd8"},"outputs":[],"source":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}